# AUTOGENERATED! DO NOT EDIT! File to edit: ../../10_npg_data.ipynb.

# %% auto 0
__all__ = ['HOST', 'DATASET', 'files', 'pipe', 'get_dataset_attachments', 'parse_attachment_content',
           'build_attachment_dataframe']

# %% ../../10_npg_data.ipynb 3
import json
import zipfile
from typing import Callable

# import more_itertools as mit
import pandas as pd
import requests as rq
from fastcore.all import *

import jupyter_playground.core as core
from ..core import DownloadContent, cache

if in_jupyter():
    from tqdm.notebook import tqdm

# %% ../../10_npg_data.ipynb 5
HOST = "northernpowergrid"
DATASET = "primary-operational-metering"


def _always_true(_):
    return True


def get_dataset_attachments(host: str, dataset: str, filter: Callable = _always_true):
    q = f"https://{host}.opendatasoft.com/api/v2/catalog/datasets/{dataset}/attachments"
    res = rq.get(q)
    res.raise_for_status()

    return L(a for a in res.json()["attachments"] if filter(a))


files = get_dataset_attachments(
    HOST, DATASET, lambda x: x["href"].endswith("_zip")
).map(lambda x: x["href"])
files.map(lambda x: x[-10:])

# %% ../../10_npg_data.ipynb 9
pipe = core.IncrementalPipeline("npg_etl", funcs=[core.attachment_download])
pipe

# %% ../../10_npg_data.ipynb 11
@cache.cache
def parse_attachment_content(
    Download: DownloadContent, max_file_size: Union[int, float] = 4e9
) -> dict:
    with zipfile.ZipFile(io.BytesIO(Download.content)) as f:
        if f.testzip() is not None:
            raise ValueError("Content byte string is not a zipfile")
        n = f.infolist()
        if len(n) != 1:
            raise ValueError("Expecting only one file but found the following:", n)
        if n[0].file_size > int(max_file_size):
            raise ValueError(f"{n[0]} exceeds max_file_size ({int(max_file_size)})")
        file_txt = (
            zipfile.Path(f, at=n[0].filename)
            .read_text(encoding="utf-8", errors="ignore")
            .replace("[,", "[", 1)  # this is much quicker than regexing
        )
    # remove newlines and whitespace
    return DownloadContent(file_txt)


pipe.append_func(parse_attachment_content)

# %% ../../10_npg_data.ipynb 13
_cat_cols = ["substation", "circuit", "unit", "description"]


# @cache.cache
def build_attachment_dataframe(text: DownloadContent) -> pd.DataFrame:
    df = (
        pd.concat(  # together the timeseries arrays
            [pd.json_normalize(t) for t in json.loads(text.content)["timeseries"]],
            axis=0,
            ignore_index=True,
        )
        .rename({"values": "data"}, axis=1)
        .assign(unit=lambda d: d.unit.replace(r"^\s*$", "deg", regex=True))
        .set_index(_cat_cols)
        .groupby(_cat_cols, sort=False)  # no sort for performance
        .apply(lambda d: pd.json_normalize(d.iat[0, -1]))  # run the normalise
        .reset_index(-1, drop=True)  # discard index from second parse
        .reset_index(drop=False)  # clear rest
        .assign(  # fix up types
            timestamp=lambda d: pd.to_datetime(d.timestamp),
            value=lambda d: pd.to_numeric(d.value, downcast="float"),
            yyyy_mm=lambda d: d.timestamp.dt.strftime("%Y-%m"),
        )
    )
    d = df.yyyy_mm.value_counts()  # remove values from neighbour months
    if len(d) != 1:
        print(text, d.__repr__())  # log these for reference

    df[_cat_cols] = df[_cat_cols].astype("category")  # categorise
    return df[d.index[d.argmax()] == df.yyyy_mm]


pipe.append_func(build_attachment_dataframe)
