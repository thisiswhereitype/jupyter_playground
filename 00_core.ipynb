{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "> Reusable utils for download caching and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import dataclasses\n",
    "import sys\n",
    "from typing import Any, Callable, Union, List\n",
    "\n",
    "\n",
    "import joblib\n",
    "import joblib.memory\n",
    "import requests as rq\n",
    "from fastcore.all import *\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and cache utils\n",
    "> Setup a `joblib` cache and create some containers to work nicely with downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DownloadContent:\n",
    "    \"\"\"\n",
    "    Masks the __repr__ with the content's hash to avoid serialising a large string.\n",
    "    This is useful to stop joblib serialising large json files.\n",
    "    \"\"\"\n",
    "\n",
    "    content: bytes\n",
    "\n",
    "    def __repr__(self):\n",
    "        return joblib.hash(self.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "class MemoryStandin:\n",
    "    def cache(self, func):\n",
    "        return func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "if __name__ != \"__main__\":\n",
    "    if sys.platform == \"linux\":\n",
    "        cache = joblib.Memory(\n",
    "            \"/mnt/d/.joblib\", verbose=0, compress=True, bytes_limit=int(200e9)\n",
    "        )\n",
    "    if sys.platform == \"win32\":\n",
    "        cache = joblib.Memory(\n",
    "            \"D:\\.joblib\", verbose=0, compress=True, bytes_limit=int(200e9)\n",
    "        )\n",
    "    cache.reduce_size()\n",
    "else:\n",
    "    if in_jupyter():\n",
    "        cache = joblib.Memory(verbose=1, compress=True)\n",
    "    else:\n",
    "        cache = MemoryStandin()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attachment Download wraps the content of a request for the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "@cache.cache\n",
    "def attachment_download(href):\n",
    "    res = rq.get(href)\n",
    "    res.raise_for_status()\n",
    "    return DownloadContent(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class IncrementalPipeline:\n",
    "    \"\"\"\n",
    "    A class whose instances can dynamically store functions.\n",
    "    When used as a callable i.e. `pipe(*args)` the functions are called in turn and returned objects\n",
    "    are passed as args into each successive function. Results are wrapped as tuples if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, funcs: List[Callable] = None) -> None:\n",
    "        self._name = name\n",
    "        self._funcs = L(funcs)\n",
    "        self._fuse_cache = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{IncrementalPipeline.__name__}(name='{self._name}', _funcs={[f.__name__  for f in self._funcs]})\"\n",
    "\n",
    "    def decorate_func(self, func):\n",
    "        self.append_func(func)\n",
    "        return func\n",
    "\n",
    "    def append_func(self, *funcs: List[Callable]):\n",
    "        self._funcs += funcs\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, slice, Iterable]):\n",
    "        return IncrementalPipeline(self._name, self._funcs[idx])\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        *args: list[Any],\n",
    "        tqdm_position: int | None = 0,\n",
    "        **init_kwargs: dict[Any, Any],\n",
    "    ):\n",
    "        try:\n",
    "            for i, f in tqdm(enumerate(self._funcs), leave=False, position=tqdm_position):\n",
    "                if not i:\n",
    "                    res = f(*args, **init_kwargs)\n",
    "                else:\n",
    "                    res = f(*res)\n",
    "                res = res if is_listy(res) else (res,)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            raise ValueError(f'{i}: When calling \"{f}\", args: {args}') from e\n",
    "\n",
    "        if isinstance(res, (tuple, fastuple)):\n",
    "            # unwrap redundant tuple\n",
    "            return res if res.__len__() > 1 else res[0]\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    @delegates(tqdm)\n",
    "    def run_group(\n",
    "        self,\n",
    "        job_args: Iterable[Tuple[Any]],\n",
    "        init_kwargs: Dict[str, Any] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        with tqdm(enumerate(job_args), **{**dict(desc=\"Jobs\"), **kwargs}) as ti:\n",
    "            for _, arg in ti:\n",
    "                yield self(\n",
    "                    *arg,\n",
    "                    **{\n",
    "                        **dict(\n",
    "                            init_kwargs=dict() if init_kwargs is None else init_kwargs,\n",
    "                            tqdm_position=1,\n",
    "                        ),\n",
    "                        **kwargs,\n",
    "                    },\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example pipeline and usage \n",
    "\n",
    "(NB: we add it twice to the pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncrementalPipeline(name='Eg', _funcs=['add_one', 'add_one'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = IncrementalPipeline(\"Eg\")\n",
    "\n",
    "\n",
    "@p.decorate_func\n",
    "def add_one(a, **_):\n",
    "    return a + 1\n",
    "\n",
    "\n",
    "p.append_func(add_one)\n",
    "p\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicers can be used to take certain stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[:1](1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs: 10it [00:00, 103.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "list(range(10)), list(p[:1].run_group(L(range(10)).map(fastuple)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
